{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ-C2Dgetg37"
   },
   "source": [
    "# **Homework 6 - Generative Adversarial Network**\n",
    "\n",
    "This is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n",
    "\n",
    "\n",
    "In this homework, you are required to build a generative adversarial  network for anime face generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTBkY5QFf3QM"
   },
   "source": [
    "## Set up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7y4wyYdEABR"
   },
   "source": [
    "### Packages Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjfM46dtmxXj"
   },
   "source": [
    "## Random seed\n",
    "Set the random seed to a certain value for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWuecW1imz42"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCTPz2iRQmwe"
   },
   "source": [
    "## Import Packages\n",
    "First, we need to import packages that will be used later.\n",
    "\n",
    "Like hw3, we highly rely on **torchvision**, a library of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TC8RRsX0QhL-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYjZ_G83_YX4"
   },
   "source": [
    "## Dataset\n",
    "1. Resize the images to (64, 64)\n",
    "1. Linearly map the values from [0, 1] to  [-1, 1].\n",
    "\n",
    "Refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ6d0_cr8R26"
   },
   "outputs": [],
   "source": [
    "class CrypkoDataset(Dataset):\n",
    "    def __init__(self, fnames, transform):\n",
    "        self.transform = transform\n",
    "        self.fnames = fnames\n",
    "        self.num_samples = len(self.fnames)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.fnames[idx]\n",
    "        # 1. Load the image\n",
    "        img = torchvision.io.read_image(fname)\n",
    "        # 2. Resize and normalize the images using torchvision.\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "def get_dataset(root):\n",
    "    fnames = glob.glob(os.path.join(root, '*'))\n",
    "    # 1. Resize the image to (64, 64)\n",
    "    # 2. Linearly map [0, 1] to [-1, 1]\n",
    "    compose = [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ]\n",
    "    transform = transforms.Compose(compose)\n",
    "    dataset = CrypkoDataset(fnames, transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGwdVhOKSjLY"
   },
   "source": [
    "### Show some images\n",
    "Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'C:/Users/BiXY/OneDrive - 厦门大学(马来西亚分校)/科研（RA）/火炉实验室/pytorch/data'\n",
    "print(os.path.join(path, 'faces'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(os.path.join(path, 'faces'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34mVNtHn7cwF",
    "outputId": "5a657b52-ce4d-4f5a-e5a8-d15d0408be5f"
   },
   "outputs": [],
   "source": [
    "images = [dataset[i] for i in range(16)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhxUjRUuHdti",
    "outputId": "b8277a3b-fe40-485c-c004-1652dba27dd3"
   },
   "outputs": [],
   "source": [
    "images = [(dataset[i]+1)/2 for i in range(16)]\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkvZ4JgCHCZD"
   },
   "source": [
    "## Model\n",
    "Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n",
    "\n",
    "The `N` of the input/output shape stands for the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0I1jRd6HFmm"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (batch, in_dim)\n",
    "    Output shape: (batch, 3, 64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, feature_dim=64):\n",
    "        super().__init__()\n",
    "    \n",
    "        #input: (batch, 100)\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            self.dconv_bn_relu(feature_dim * 8, feature_dim * 4),               #(batch, feature_dim * 16, 8, 8)     \n",
    "            self.dconv_bn_relu(feature_dim * 4, feature_dim * 2),               #(batch, feature_dim * 16, 16, 16)     \n",
    "            self.dconv_bn_relu(feature_dim * 2, feature_dim),                   #(batch, feature_dim * 16, 32, 32)     \n",
    "        )\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2,\n",
    "                               padding=2, output_padding=1, bias=False),\n",
    "            nn.Tanh()   \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def dconv_bn_relu(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2,\n",
    "                               padding=2, output_padding=1, bias=False),        #double height and width\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2(y)\n",
    "        y = self.l3(y)\n",
    "        return y\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (batch, 3, 64, 64)\n",
    "    Output shape: (batch)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, feature_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "            \n",
    "        #input: (batch, 3, 64, 64)\n",
    "        \"\"\"\n",
    "        NOTE FOR SETTING DISCRIMINATOR:\n",
    "\n",
    "        Remove last sigmoid layer for WGAN\n",
    "        \"\"\"\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.conv_bn_lrelu(feature_dim, feature_dim * 2),                   #(batch, 3, 16, 16)\n",
    "            self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4),               #(batch, 3, 8, 8)\n",
    "            self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8),               #(batch, 3, 4, 4)\n",
    "            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),\n",
    "            #nn.Sigmoid() \n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "    def conv_bn_lrelu(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        NOTE FOR SETTING DISCRIMINATOR:\n",
    "\n",
    "        You can't use nn.Batchnorm for WGAN-GP\n",
    "        Use nn.InstanceNorm2d instead\n",
    "        \"\"\"\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, 4, 2, 1),\n",
    "            #nn.BatchNorm2d(out_dim),\n",
    "            nn.InstanceNorm2d(out_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cxo4teqaO5RJ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5sCGIUtSViC"
   },
   "source": [
    "### Initialization\n",
    "- hyperparameters\n",
    "- model\n",
    "- optimizer\n",
    "- dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EqomOouHezf"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "z_sample = Variable(torch.randn(100, z_dim)).cuda()\n",
    "lr = 1e-4\n",
    "\n",
    "\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n",
    "n_epoch = 50 # 50\n",
    "n_critic = 2 # 5\n",
    "# clip_value = 0.01\n",
    "\n",
    "log_dir = os.path.join(path, 'logs')\n",
    "ckpt_dir = os.path.join(path, 'checkpoints')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Model\n",
    "G = Generator(in_dim=z_dim).cuda()\n",
    "D = Discriminator(3).cuda()\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "# Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
    "# Optimizer\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\n",
    "# opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,   # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
    "                 resize_input=True,\n",
    "                 normalize_input=True,\n",
    "                 requires_grad=False):\n",
    "        \n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \\\n",
    "            'Last possible output block index is 3'\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        \n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x,\n",
    "                              size=(299, 299),\n",
    "                              mode='bilinear',\n",
    "                              align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp\n",
    "    \n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx])\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
    "                    cuda=False):\n",
    "    model.eval()\n",
    "    act=np.empty((len(images), dims))\n",
    "    \n",
    "    if cuda:\n",
    "        batch=images.cuda()\n",
    "    else:\n",
    "        batch=images\n",
    "    pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "    mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
    "    mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
    "    \n",
    "    \"\"\"get fretched distance\"\"\"\n",
    "    fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "    return fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写gp函数并添加进loss_D \n",
    "from torch import autograd\n",
    "def gp(D,r_imgs, f_imgs):\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    alpha = Tensor(np.random.random((r_imgs.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha*r_imgs + (1 - alpha)*f_imgs).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(r_imgs.shape[0]).fill_(1.0), requires_grad=False)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(1, dim=1) - 1)**2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpJA1wzi0tii"
   },
   "source": [
    "### Training loop\n",
    "We store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678,
     "referenced_widgets": [
      "565d67d9e799493fa2197dda1d06c4ea",
      "53faf655a44d43abb5b1df6a5724917a",
      "1806f82990df4176a0c62425baf83311",
      "a478d938977740079eda33017c5e3c10",
      "09d49355e7414445bee8e9a4b521bfe7",
      "40710bba15b64b9ab454a2d5ad630e03",
      "0dc308b6988f413fb12fa046c39a0c4e",
      "034c8ea9a887489f8c5ba191035255f2"
     ]
    },
    "id": "dgkqPih1o5Az",
    "outputId": "29a89ab4-4e6a-4ebf-e845-0d5ee3b1a424",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "fretchet_dists = []\n",
    "steps = 0\n",
    "for e, epoch in enumerate(range(n_epoch)):\n",
    "    progress_bar = tqdm(dataloader)\n",
    "    for i, data in enumerate(progress_bar):\n",
    "        imgs = data\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train D\n",
    "        # ============================================\n",
    "        z = Variable(torch.randn(bs, z_dim)).cuda()\n",
    "        r_imgs = Variable(imgs).cuda()\n",
    "        f_imgs = G(z)\n",
    "        # print(f_imgs)\n",
    "\n",
    "        \"\"\" Use WGAN Loss and plus penalty \"\"\"\n",
    "        # Label\n",
    "        r_label = torch.ones((bs)).cuda()\n",
    "        f_label = torch.zeros((bs)).cuda()\n",
    "\n",
    "        # Model forwarding\n",
    "        r_logit = D(r_imgs.detach())\n",
    "        f_logit = D(f_imgs.detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = criterion(r_logit, r_label)\n",
    "        # f_loss = criterion(f_logit, f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        gradient_penalty = gp(D, r_imgs, f_imgs)\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty\n",
    "        \n",
    "       \n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "\n",
    "        # for p in D.parameters():\n",
    "        #    p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        if steps % n_critic == 0:\n",
    "            # Generate some fake images.\n",
    "            z = Variable(torch.randn(bs, z_dim)).cuda()\n",
    "            f_imgs = G(z)\n",
    "\n",
    "            # Model forwarding\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            \"\"\" Use WGAN Loss\"\"\"\n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G = criterion(f_logit, r_label)\n",
    "            # WGAN Loss\n",
    "            loss_G = -torch.mean(D(f_imgs))\n",
    "\n",
    "            # Model backwarding\n",
    "            G.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "\n",
    "        steps += 1\n",
    "        \n",
    "        # Set the info of the progress bar\n",
    "        #   Note that the value of the GAN loss is not directly related to\n",
    "        #   the quality of the generated images.\n",
    "        print({\n",
    "            'Loss_D': round(loss_D.item(), 4),\n",
    "            'Loss_G': round(loss_G.item(), 4),\n",
    "            'Epoch': e+1,\n",
    "            'Step': steps,\n",
    "        })\n",
    "    D_losses.append(loss_D.item())\n",
    "    G_losses.append(loss_G.item())\n",
    "    fretchet_dist=calculate_fretchet(r_imgs,f_imgs,model) \n",
    "    fretchet_dists.append(fretchet_dist)\n",
    "    print({\n",
    "    'Fretchet_Distance': round(fretchet_dist, 4),\n",
    "        })\n",
    "    G.eval()\n",
    "    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
    "    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
    "    print(f' | Save some samples to {filename}.')\n",
    "    \n",
    "    # Show generated images in the jupyter notebook.\n",
    "    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    G.train()\n",
    "\n",
    "    if (e+1) % 5 == 0 or e == 0:\n",
    "        # Save the checkpoints.\n",
    "        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n",
    "        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"FID During Training\")\n",
    "plt.plot(fretchet_dists)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"FID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training(WGAN-GP)\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2uJFmTtKBeH"
   },
   "source": [
    "## Inference\n",
    "Use the trained model to generate anime faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXPXcVD_HJB2"
   },
   "source": [
    "### Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JnQdNx2SUS2",
    "outputId": "3148dfab-79e4-480a-e740-bdd81ad53380"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "G = Generator(z_dim)\n",
    "G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\n",
    "G.eval()\n",
    "G.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I8PDocbHQiN"
   },
   "source": [
    "### Generate and show some images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-SYKrRea_-Q",
    "outputId": "aba269bb-3ea7-4df9-a30a-569107f522c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate 1000 images and make a grid to save them.\n",
    "n_output = 50\n",
    "z_sample = Variable(torch.randn(n_output, z_dim)).cuda()\n",
    "imgs_sample = (G(z_sample).data + 1) / 2.0\n",
    "log_dir = os.path.join(path, 'logs')\n",
    "filename = os.path.join(log_dir, 'result.jpg')\n",
    "torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
    "\n",
    "# Show 32 of the images.\n",
    "grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(grid_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B04ATOTHc4F"
   },
   "source": [
    "### Compress the generated images using **tar**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbcmoTQpz_yf",
    "outputId": "c2ef944b-b701-47b2-94b8-80a7aab36163"
   },
   "outputs": [],
   "source": [
    "# Save the generated images.\n",
    "os.makedirs('output_WGANGP', exist_ok=True)\n",
    "for i in range(50):\n",
    "    torchvision.utils.save_image(imgs_sample[i], f'output_WGANGP/{i+1}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Y7y4wyYdEABR",
    "2NFjuZTPDxLn"
   ],
   "name": "hw6_GAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034c8ea9a887489f8c5ba191035255f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09d49355e7414445bee8e9a4b521bfe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dc308b6988f413fb12fa046c39a0c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1806f82990df4176a0c62425baf83311": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40710bba15b64b9ab454a2d5ad630e03",
      "placeholder": "​",
      "style": "IPY_MODEL_09d49355e7414445bee8e9a4b521bfe7",
      "value": "100.0%"
     }
    },
    "40710bba15b64b9ab454a2d5ad630e03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53faf655a44d43abb5b1df6a5724917a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "565d67d9e799493fa2197dda1d06c4ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1806f82990df4176a0c62425baf83311",
       "IPY_MODEL_a478d938977740079eda33017c5e3c10"
      ],
      "layout": "IPY_MODEL_53faf655a44d43abb5b1df6a5724917a"
     }
    },
    "a478d938977740079eda33017c5e3c10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_034c8ea9a887489f8c5ba191035255f2",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0dc308b6988f413fb12fa046c39a0c4e",
      "value": 100
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
