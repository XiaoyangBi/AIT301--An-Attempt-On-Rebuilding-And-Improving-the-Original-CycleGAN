# AIT301--An Attempt On Rebuilding And Improving the Original CycleGAN

Style transfer is a heated computer vision topic that has been broadly mentioned in both academic research and practical applications, for example, the style transfer technique is behind the various filters in daily used image processing tools. However, it had been a major drawback in this field that its training set required strictly paired images, until the introduction of CycleGAN, a combination of **style transfer technique** and **Generating Adversarial Networks (GAN)**, which has also been one of the most popular technologies in machine learning since its advent. **CycleGAN** is an image style transfer technique proposed by **Jun-Yan Zhu et al. at the University of California, Berkeley**. The paper proposed a way that can be performed without the strict pairs of the source image and style image so that a wider range of dataset options can be incorporated into this field. The generator in the CycleGAN network consists of an encoder, converter, and decoder, which can play the role of preserving the original image features and converting images. In our research, we found that there is still certain scope for improvement in the original model, so we conducted a further study on the basis of rebuilding the original CycleGAN. We studied the CycleGAN framework, which is an unsupervised learning style transfer model based on generative adversarial networks, and **proposed an improved nerual network model by modifying the adversarial loss, and compared it with the original CycleGAN network.** The results showed that **the modified CycleGAN can obtain more realistic images with better visual effects than original CycleGAN after training in fewer rounds.**
